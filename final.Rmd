---
title: "STP 494 Final"
author: "Jiaqi Wu, Excel Ortega, Terry Wen, Alex Ryan, Peter Le"
date: "April 22, 2018"
output: html_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 
This project will attempt to classify breast cancer tumors into two categories, benign or malignant, depending on tumor characteristics. 

## Data
The Wisconsin Diagnostic Breast Cancer (WDBC) dataset was obtained from [Kaggle](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data). Attributes include ID number, diagnosis (B = benign, M = malignant), and 30 real-valued input features. This project will focus on 11/32 attributes given in this dataset.

Features are computed from a digitized image of breast mass, and the data describes characteristics of the cell nuclei present in the image. 

Variable Name:  | Variable Description:
---------------| ---------------------------------------------------------------------------
`radius`         | Mean of distances from center to points on the perimeter 
`texture`         | Standard deviation of gray-scale values
`perimeter`         | Measurement of cell boundary
`area`         | Size of the cell's 2D surface
`smoothness`         | Local variation in radius lengths
`compactness`         | Perimeter^2 / Area - 1.0
`concavity`         | Severity of concave portions of the contour
`concave points`         | Number of concave portions of the contour 
`symmetry`         | Similarity of parts around an axis
`fractal dimension`         | "Coastline approximation" - 1

The mean, standard error, and "worst" (largest) of each of the 10 features were computed, resulting in 30 attributes total. This project only looks at the "mean" values.


### Processing the Data 
```{r echo=FALSE, include=FALSE, warning=FALSE}
## prep and setup data 
getwd()
# wdbc = wisconsin diagnostic breast cancer 
wdbc = read.csv("brcancer.csv", header=TRUE)
wdbc = wdbc[,-1] #remove ID from dataset
wdbc = wdbc[1:11] #use means only

#wdbc$diagnosis2 = ifelse(wdbc$diagnosis == "M", 1, 0)

## Split into 70% train, 30% test 
n = nrow(wdbc) #num "rows" in data
set.seed(30) 
ntrain = floor(n*0.70) #70% for train 
ii = sample(1:n,ntrain) #reorder elements randomly
wdbc_train = wdbc[ii,]
wdbc_test= wdbc[-ii,]

diagnosis2_train = ifelse(wdbc_train$diagnosis == "M", 1, 0)
diagnosis2_test = ifelse(wdbc_test$diagnosis == "M", 1, 0)

```

First, we removed the ID attribute from the dataset because it does not correlate with diagnosis. Then, we created a dataset with only the variables we wantd to use: columns 1 through 11 (diagnosis column and 10 mean-value features). 

We also added a column that maps B (benign) to 0 and M (malignant) to 1. This column contains the same information as the original diagnosis column, except the values are 0 and 1 rather than B and M (respectively). In total, the dataset contains 569 observations and 12 variables. We split the data into two sets to build our models: 70% training data, and 30% test data. 

## Data Models 
We attempted three methods from our Machine Learning class: logistic regression, k-nearest neighbors, and neural nets. Logistic regression was our baseline reference, but the remaining models were tuned for optimal performance. 


### Logistic Regression
We used generalized linear models (glm) to model our data. Since the outcome we are trying to predict is binomial (benign or malignant), we chose a logistic regression model, given below: 

``` {r echo = FALSE, include= TRUE, eval = TRUE}

#save labels for later
wdbc_testy = diagnosis2_test
wdbc_trainy = diagnosis2_train

#model the data using glm
logit.model = glm(diagnosis2_train ~ ., data = wdbc_train[-c(1)]) 
summary(logit.model)
```

Confusion Matrix: 
```{r echo=FALSE, include = TRUE, eval=TRUE}

logityhat = predict(logit.model,wdbc_test)

#split values by probability
yhatbin = ifelse(logityhat > 0.5, 1, 0)

#get confusion matrix
(tabl = table(yhatbin, wdbc_testy))
logit_accuracy = (tabl[1,1]+tabl[2,2])/nrow(wdbc_test) *100
```

This model gives us an accuracy of `r logit_accuracy`%. 

```{r echo = FALSE, eval = TRUE, fig.width=5, fig.height=5, fig.align='center'}
#plot distributed values
plot(logityhat,wdbc_testy)
```

**Figure X**: Shows the probabilities of diagnosis predicted by our logistic model, versus the actual diagnosis. When computing accuracy, values greater than 0.5 were assigned a value of 1 (malignant), while values less than or equal to 0.5 were assigned a value of 0 (benign).  

### kNN 
We used the r package **kknn** to fit our knn model. To begin, we ran cross-validation once with a large interval of k-values to find an appropriate range of values. Then, with a smaller interval, we ran 10-fold cross validation multiple times to find the optimal k-value. We fit our test data using the optimal k-value, and generated plots and a confusion matrix to determine accuracy. 

```{r echo=FALSE, include=FALSE, eval=TRUE}

# get libraries and helper files 
library(kknn)
source("http://www.rob-mcculloch.org/2018_ml/webpage/R/docv.R")

# assign variables as x and y
x=wdbc_train[,2:11] #only use mean data; 2:11
y=wdbc_train[,1]

# apply scaling function to each column of x
mmsc=function(x){return((x-min(x))/(max(x)-min(x)))}
xs = apply(x,2,mmsc) #scaling function 

#plot y vs each x
#par(mfrow=c(1,2)) #two plot frames
#pdf(file="crossval-test.pdf")
#plot(x[,1],y,xlab="radius_mean",ylab="diagnosis")
#dev.off()

# benign = 0
# malignant = 1 
y = ifelse(y == "M", 1, 0)

# turn y into a vector, yv, for cross-val 
yv = c(y)

```


```{r echo=FALSE, include=FALSE}
#run cross val once
par(mfrow=c(1,1)) 
#set.seed(30) 
kv = seq(1, 100, 5) #k values to try
n = length(yv)
cvtemp = docvknn(xs,yv,kv,nfold=10)
cvtemp = sqrt(cvtemp/n) #docvknn returns sum of squares
```

```{r eval=TRUE, echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
plot(kv,cvtemp)
```

**Figure x:** Comparing the k parameter from 1 to 100 in intervals of 5 with the RMSE of a single model. This allows us to approximate range of appropriate k-values before moving onto a more computationally intensive model. 

It looks like the optimal k-value is somewhere around k = 19, so we will reset the kv-value to a smaller window (2 to 30) for 10-fold cross validation.

```{r echo=FALSE, include=FALSE, eval=TRUE}

kv = 2:30 #new k values to try

#run cross val multiple times 
cvmean = rep(0,length(kv))
ndocv = 10 # number of CV splits to try
n=length(yv) #vector version of y
cvmat = matrix(0,length(kv),ndocv) # track results for each split
for(i in 1:ndocv) {
    cvtemp = docvknn(xs, yv, kv, nfold=10)
    cvmean = cvmean + cvtemp
    cvmat[,i] = sqrt(cvtemp/n)
}

```



```{r echo=FALSE, fig.width=5, fig.height=5, fig.align='center'}
cvmean = cvmean/ndocv
cvmean = sqrt(cvmean/n)
plot(kv,cvmean,type="n",ylim=range(cvmat),xlab="k",cex.lab=1.5)
for(i in 1:ndocv) lines(kv,cvmat[,i],col=i,lty=3)  #plot each result
lines(kv,cvmean,type="b",col="black",lwd=2)  #plot average result

```

**Figure X:** This plot compares the paramter k at values between 2 and 30 with the RMSE of each model. Each colored line represents a different subset of the data; the black line represents the average values. This helps us find the balance between bias and variance for the model. 


```{r echo =FALSE,eval=TRUE} 
#print table of k's and rmse's
for(i in 2:29) {
  cat("k=",i, " rmse=", cvmean[i], "\n")  
} 

#cat("optimal k=", which.min(cvmean), "with average rmse of ", min(cvmean), "\n")

```


Our optimal k-value is `r which.min(cvmean)` with an average RMSE of `r min(cvmean)`. This is pretty close to our initial approximation. Let's refit using k = 17:


```{r echo=FALSE, eval=TRUE}
near17 = kknn(diagnosis~.,wdbc_train,wdbc_test,k=17,kernel="rectangular")
#fmat = cbind(wdbc_test$diagnosis,near17$prob[,2])

(tabl = table(near17$fitted, wdbc_test$diagnosis))
knn_accuracy = (tabl[1,1]+tabl[2,2])/nrow(wdbc_test) *100

```

The confusion matrix shows that kNN performs decently well, with `r knn_accuracy`% accuracy.  

```{r fig.width=5, fig.height=5, fig.align='center'}

plot(near17$prob[,2], wdbc_testy)

```

**Figure X:** Probabilities predicted by our kNN model versus actual diagnosis. 

### Neural Nets

Single layer neural net with 5 nodes.

```{r echo=FALSE, eval=TRUE}
#Load NN library and set seed
library(nnet)
#set.seed(30)

#Generate model and yhat values for test data
nnmodel = nnet(diagnosis ~ ., wdbc_train,size=5,decay=.01,maxit=1000)
nnyhat = predict(nnmodel, wdbc_test)

#Convert to binary values
nnyhat_bin = ifelse(nnyhat > 0.5, 1, 0)

#Confusion Matrix
tabl = table(nnyhat_bin, wdbc_testy)
nn_accuracy = (tabl[1,1]+tabl[2,2])/nrow(wdbc_test) *100

```

Our neural nets model has an accuracy of `r nn_accuracy`%. While this percentage is still decent, we are concerned because the errors are more widespread (i.e., incorrectly predicted malignant points have values very close to 0, and vice versa).

``` {r echo = FALSE, eval = TRUE, fig.width=5, fig.height=5, fig.align='center'}

plot(nnyhat, wdbc_testy)

```
**Figure X:** Probabilities predicted by neural nets model versus actual diagnosis. 


## Conclusion
``` {r echo=FALSE, include = TRUE, eval=TRUE}
#plot all models
fmat = cbind(wdbc_testy,logityhat,near17$prob[,2],nnyhat)
colnames(fmat) = c("y","logit","kNN17","nn")
pairs(fmat)

```
**Figure X:** All models versus real diagnosis values. 
